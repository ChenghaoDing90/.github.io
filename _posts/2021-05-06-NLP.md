---
title: Natural Language Processing
author: Chenghao Ding
layout: post
comments: true
---

# Natural Languague Processing Exercise

* Questions: Identify the spam emails from hams?

## Natural Language Processing 
NLP is a branch of Artificial Intelligence which deal with bridging the machines understanding humans in their Natural Language. Natural Language can be in form of text or sound, which are used for humans to communicate each other. NLP can enable humans to communicate to machines in a natural way.

## Text Classification
Text Classification is a process involved in Sentiment Analysis. It is classification of peoples opinion or expressions into different sentiments. Sentiments include Positive, Neutral, and Negative, Review Ratings and Happy, Sad. Sentiment Analysis can be done on different consumer centered industries to analyse people's opinion on a particular product or subject.

In this project, several datasets are going to be used to show how different machine learning model can be applied to various applications in NLP, such as text classification and sentiment analysis.

### 1. Loading Data

<div class="fig figcenter fighighlight">
  <img src="/assets/images/nlp/Capture4445.PNG" width="800" height="300">
  <div class="figcaption"><br>
  </div>
</div>

<div class="fig figcenter fighighlight">
  <img src="/assets/images/nlp/Capture66.PNG" width="800" height="300">
  <div class="figcaption"><br>
  </div>
</div>

### 2. Exploratory Data Analysis

<div class="fig figcenter fighighlight">
  <img src="/assets/images/nlp/data_dist.png" width="800" height="500">
  <div class="figcaption"><br>
  </div>
</div>

<div class="fig figcenter fighighlight">
  <img src="/assets/images/nlp/data_role.png" width="800" height="500">
  <div class="figcaption"><br>
  </div>
</div>

### 3. Data Pre-processing

#### 3.1 Cleaning the corpus

<div class="fig figcenter fighighlight">
  <img src="/assets/images/nlp/Capturedd.PNG" width="1200" height="300">
  <div class="figcaption"><br>
  </div>
</div>

##### Remove Stopwords 
Stopwords are commonly used words in English which have no contextual meaning in an sentence. So therefore we remove them before classification. Some examples removing stopwords are: 

<div class="fig figcenter fighighlight">
  <img src="/images/Captureee.PNG" width="1200" height="300">
  <div class="figcaption"><br>
  </div>
</div>

#### 3.2 Stemming/Lemmatization
For grammatical reasons, documents are going to use different forms of a word, such as write, writing and writes. Additionally, there are families of derivationally related words with similar meanings. The goal of both stemming and lemmatization is to reduce inflectional forms and sometimes derivationally related forms of a word to a common base form.

Stemming usually refers to a process that chops off the ends of words in the hope of achieving goal correctly most of the time and often includes the removal of derivational affixes.

Lemmatization usually refers to doing things properly with the use of a vocabulary and morphological analysis of words, normally aiming to remove inflectional endings only and to return the base and dictionary form of a word.

As far as the meaning of the words is not important for this study, we will focus on stemming rather than lemmatization.

Stemming algorithms
There are several stemming algorithms implemented in NLTK Python library:

PorterStemmer uses Suffix Stripping to produce stems. PorterStemmer is known for its simplicity and speed. Notice how the PorterStemmer is giving the root (stem) of the word "cats" by simply removing the 's' after cat. This is a suffix added to cat to make it plural. But if you look at 'trouble', 'troubling' and 'troubled' they are stemmed to 'trouble' because PorterStemmer algorithm does not follow linguistics rather a set of 05 rules for different cases that are applied in phases (step by step) to generate stems. This is the reason why PorterStemmer does not often generate stems that are actual English words. It does not keep a lookup table for actual stems of the word but applies algorithmic rules to generate stems. It uses the rules to decide whether it is wise to strip a suffix.
One can generate its own set of rules for any language that is why Python nltk introduced SnowballStemmers that are used to create non-English Stemmers!
LancasterStemmer (Paice-Husk stemmer) is an iterative algorithm with rules saved externally. One table containing about 120 rules indexed by the last letter of a suffix. On each iteration, it tries to find an applicable rule by the last character of the word. Each rule specifies either a deletion or replacement of an ending. If there is no such rule, it terminates. It also terminates if a word starts with a vowel and there are only two letters left or if a word starts with a consonant and there are only three characters left. Otherwise, the rule is applied, and the process repeats.

<div class="fig figcenter fighighlight">
  <img src="/images/Capturegg.PNG" width="1200" height="300">
  <div class="figcaption"><br>
  </div>
</div>

#### 3.3 Target encoding

<div class="fig figcenter fighighlight">
  <img src="/images/Capturekk.PNG" width="1200" height="300">
  <div class="figcaption"><br>
  </div>
</div>

<div class="fig figcenter fighighlight">
  <img src="/images/Capturecc.PNG" width="1200" height="300">
  <div class="figcaption"><br>
  </div>
</div>

<div class="fig figcenter fighighlight">
  <img src="/assets/images/nlp/wordclo1.png" width="1200" height="300">
  <div class="figcaption"><br>
  </div>
</div>

<div class="fig figcenter fighighlight">
  <img src="/images/wordclo2.png" width="1200" height="300">
  <div class="figcaption"><br>
  </div>
</div>

### 4. Tokens visualization

### 5. Vectorization

Currently, we have the messages as lists of tokens (also known as lemmas) and now we need to convert each of those messages into a vector the SciKit Learn's algorithm models can work with.

We'll do that in three steps using the bag-of-words model:

Count how many times does a word occur in each message (Known as term frequency)
Weigh the counts, so that frequent tokens get lower weight (inverse document frequency)
Normalize the vectors to unit length, to abstract from the original text length (L2 norm)
Let's begin the first step:

Each vector will have as many dimensions as there are unique words in the SMS corpus. We will first use SciKit Learn's CountVectorizer. This model will convert a collection of text documents to a matrix of token counts.

We can imagine this as a 2-Dimensional matrix. Where the 1-dimension is the entire vocabulary (1 row per word) and the other dimension are the actual documents, in this case a column per text message.

#### 5.1 Tunning CountVectorizer

CountVectorizer has a few parameters you should know.

stop_words: Since CountVectorizer just counts the occurrences of each word in its vocabulary, extremely common words like ‘the’, ‘and’, etc. will become very important features while they add little meaning to the text. Your model can often be improved if you don’t take those words into account. Stop words are just a list of words you don’t want to use as features. You can set the parameter stop_words=’english’ to use a built-in list. Alternatively you can set stop_words equal to some custom list. This parameter defaults to None.

ngram_range: An n-gram is just a string of n words in a row. E.g. the sentence ‘I am Groot’ contains the 2-grams ‘I am’ and ‘am Groot’. The sentence is itself a 3-gram. Set the parameter ngram_range=(a,b) where a is the minimum and b is the maximum size of ngrams you want to include in your features. The default ngram_range is (1,1). In a recent project where I modeled job postings online, I found that including 2-grams as features boosted my model’s predictive power significantly. This makes intuitive sense; many job titles such as ‘data scientist’, ‘data engineer’, and ‘data analyst’ are 2 words long.

min_df, max_df: These are the minimum and maximum document frequencies words/n-grams must have to be used as features. If either of these parameters are set to integers, they will be used as bounds on the number of documents each feature must be in to be considered as a feature. If either is set to a float, that number will be interpreted as a frequency rather than a numerical limit. min_df defaults to 1 (int) and max_df defaults to 1.0 (float).

max_features: This parameter is pretty self-explanatory. The CountVectorizer will choose the words/features that occur most frequently to be in its’ vocabulary and drop everything else.

### 6. Build Model

<div class="fig figcenter fighighlight">
  <img src="/images/conf-matr.png" width="1000" height="500">
  <div class="figcaption"><br>
  </div>
</div>

#### 6.2 Naive Bayes

<div class="fig figcenter fighighlight">
  <img src="/images/conf-matr1.png" width="1000" height="500">
  <div class="figcaption"><br>
  </div>
</div>

#### 6.2 XGBoost

<div class="fig figcenter fighighlight">
  <img src="/images/conf-matr3.png" width="1200" height="300">
  <div class="figcaption"><br>
  </div>
</div>

<div class="fig figcenter fighighlight">
  <img src="/images/wordclo2.png" width="1200" height="300">
  <div class="figcaption"><br>
  </div>
</div>

### 7. LSTM Model

<div class="fig figcenter fighighlight">
  <img src="/images/fit1.png" width="1200" height="300">
  <div class="figcaption"><br>
  </div>
</div>

<div class="fig figcenter fighighlight">
  <img src="/images/Captureyy.PNG" width="1200" height="300">
  <div class="figcaption"><br>
  </div>
</div>

### 8. Disaster Tweets


<div class="fig figcenter fighighlight">
  <img src="/images/data_dist2.png" width="1200" height="300">
  <div class="figcaption"><br>
  </div>
</div>

<div class="fig figcenter fighighlight">
  <img src="/images/data_role2.png" width="1200" height="300">
  <div class="figcaption"><br>
  </div>
</div>

<div class="fig figcenter fighighlight">
  <img src="/images/clo3.png" width="1200" height="300">
  <div class="figcaption"><br>
  </div>
</div>

<div class="fig figcenter fighighlight">
  <img src="/images/clo4.png" width="1200" height="300">
  <div class="figcaption"><br>
  </div>
</div>

<div class="fig figcenter fighighlight">
  <img src="/images/fit2.png" width="1200" height="300">
  <div class="figcaption"><br>
  </div>
</div>


