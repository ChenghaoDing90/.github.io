---
title: Natural Language Processing
author: Chenghao Ding
layout: post
comments: true
---

# Natural Languague Processing

* Questions 1: Identify the spam emails from hams?
* Questions 2: Identify the disaster tweets from 10,000 of tweets?

In this project, several datasets are going to be used to show how different machine learning model can be applied to various applications in NLP, such as text classification and sentiment analysis.

## 1. Loading Data and Exploratory Data Analysis

<div class="fig figcenter fighighlight">
  <img src="/assets/images/nlp/Capture4445.PNG" width="600" height="200">
  <div class="figcaption"><br>
  </div>
</div>

<div class="fig figcenter fighighlight">
  <img src="/assets/images/nlp/Capture66.PNG" width="600" height="200">
  <div class="figcaption"><br>
  </div>
</div>

As we can see, the classes are imbalanced, so we can consider using some kind of resampling.

<div class="fig figcenter fighighlight">
  <img src="/assets/images/nlp/data_dist.png" width="600" height="400">
  <div class="figcaption"><br>
  </div>
</div>

As we can see, the ham message length tend to be lower than spam message length.

<div class="fig figcenter fighighlight">
  <img src="/assets/images/nlp/data_role.png" width="600" height="400">
  <div class="figcaption"><br>
  </div>
</div>

## 2. Data Pre-processing
Before we feed texts into our machine learning models, we need to clean the corpus, and remove stop words, as well as stemming or lematization. After that, encode texts into vectors.

### 2.1 Cleaning the corpus
Make text lowercase, remove text in square brackets,remove links,remove punctuation and remove words containing numbers.

<div class="fig figcenter fighighlight">
  <img src="/assets/images/nlp/Capturedd.PNG" width="1200" height="300">
  <div class="figcaption"><br>
  </div>
</div>

#### Remove Stopwords 
Stopwords are commonly used words in English which have no contextual meaning in an sentence. So therefore we remove them before classification. The figure below shows the cleaned texts after stopwords are removed.

<div class="fig figcenter fighighlight">
  <img src="/assets/images/nlp/Captureee.PNG" width="1200" height="300">
  <div class="figcaption"><br>
  </div>
</div>

### 2.2 Stemming/Lemmatization

Stemming usually refers to a process that chops off the ends of words in the hope of achieving goal correctly most of the time and often includes the removal of derivational affixes.

Lemmatization usually refers to doing things properly with the use of a vocabulary and morphological analysis of words, normally aiming to remove inflectional endings only and to return the base and dictionary form of a word.

<div class="fig figcenter fighighlight">
  <img src="/assets/images/nlp/Capturegg.PNG" width="1200" height="300">
  <div class="figcaption"><br>
  </div>
</div>

### 2.3 Target encoding
Now, the corpus have been cleaned. The figure below shows the cleaned sentences.

<div class="fig figcenter fighighlight">
  <img src="/assets/images/nlp/Capturekk.PNG" width="1200" height="300">
  <div class="figcaption"><br>
  </div>
</div>

Now, we need to label the ham as 0, but spam as 1.

<div class="fig figcenter fighighlight">
  <img src="/assets/images/nlp/Capturecc.PNG" width="1200" height="300">
  <div class="figcaption"><br>
  </div>
</div>

### 3. Tokens visualization (Word Cloud) and Vectorization

Top Words for HAM emails

<div class="fig figcenter fighighlight">
  <img src="/assets/images/nlp/wordclo1.png" width="1200" height="300">
  <div class="figcaption"><br>
  </div>
</div>

Top Words for SPAM emails

<div class="fig figcenter fighighlight">
  <img src="/assets/images/nlp/wordclo2.png" width="1200" height="300">
  <div class="figcaption"><br>
  </div>
</div>

We will first use SciKit Learn's CountVectorizer. This model will convert a collection of text documents to a matrix of token counts. Tunning CountVectorizer: CountVectorizer has a few parameters such as "stop_words", ngram_range, "min_df, max_df", "max_features".

#### Word Embeddings: GloVe
We can derive semantic relationships between words from the co-occurrence matrix. Now we will load embedding vectors of those words that appear in the Glove dictionary. Others will be initialized to 0.

## 4. Build Model

### 4.1 Naive Bayes

<div class="fig figcenter fighighlight">
  <img src="/assets/images/nlp/conf-matr1.png" width="800" height="400">
  <div class="figcaption"><br>
  </div>
</div>

Accuracy = 0.96.

### 4.2 XGBoost
Train Accuracy = 0.98. Test Accuracy = 0.96.

<div class="fig figcenter fighighlight">
  <img src="/assets/images/nlp/conf-matr3.png" width="800" height="400">
  <div class="figcaption"><br>
  </div>
</div>

### 4.3. LSTM Model
After epoch 3, the loss of validation didn't imporved. There's overfit problem.

<div class="fig figcenter fighighlight">
  <img src="/assets/images/nlp/fit1.png" width="1200" height="300">
  <div class="figcaption"><br>
  </div>
</div>

## 5. Disaster Tweets

<div class="fig figcenter fighighlight">
  <img src="/assets/images/nlp/Captureyy.PNG" width="800" height="200">
  <div class="figcaption"><br>
  </div>
</div>

### 5.1 Data Pre-Processing

<div class="fig figcenter fighighlight">
  <img src="/assets/images/nlp/data_dist2.png" width="800" height="400">
  <div class="figcaption"><br>
  </div>
</div>

<div class="fig figcenter fighighlight">
  <img src="/assets/images/nlp/data_role2.png" width="800" height="400">
  <div class="figcaption"><br>
  </div>
</div>

### 5.2 Word Cloud

Top Words for Real Disaster Tweets

<div class="fig figcenter fighighlight">
  <img src="/assets/images/nlp/clo3.png" width="1200" height="300">
  <div class="figcaption"><br>
  </div>
</div>

Top Words for Fake Disaster Tweets
<div class="fig figcenter fighighlight">
  <img src="/assets/images/nlp/clo4.png" width="1200" height="300">
  <div class="figcaption"><br>
  </div>
</div>

### 5.3 GloVe-LSTM Model
First, we use the embedding layer to map the words to their GloVe vectors, and then those vectors are input to the LSTM layers followed by a Dense layer with ‘sigmoid’ activation.

<div class="fig figcenter fighighlight">
  <img src="/assets/images/nlp/fit2.png" width="800" height="300">
  <div class="figcaption"><br>
  </div>
</div>

Accuracy: 0.81
